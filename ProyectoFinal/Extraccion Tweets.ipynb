{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extracción de tweets con Twint** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import twint\n",
    "#import nest_asyncio\n",
    "#nest_asyncio.apply()\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crear_carpeta(Rutabase, name_carpeta):\n",
    "    # Se define el nombre de la carpeta o directorio a crear\n",
    "    directorio = Rutabase + name_carpeta\n",
    "    if os.path.exists(directorio):\n",
    "        shutil.rmtree(directorio)\n",
    "        os.makedirs(directorio)\n",
    "        print(\"Se reemplazo el directorio %s \" % directorio)\n",
    "    else:\n",
    "        try:\n",
    "            os.mkdir(directorio)\n",
    "        except OSError:\n",
    "            print(\"La creación del directorio %s falló\" % directorio)\n",
    "        else:\n",
    "            print(\"Se ha creado el directorio: %s \" % directorio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Buscar_Palabra(cadena, palabra_incluida):\n",
    "    lista_cadenas = [cadena]\n",
    "    #print(cadena.find(palabra_incluida))\n",
    "    if cadena.find(palabra_incluida) < 0:\n",
    "        lista_cadenas.append(palabra_incluida)\n",
    "    return lista_cadenas\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuracion de Twint ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Se establece el codigo de configuracion para recuperacion los tweets mediante usernames:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweets(i, since, until):\n",
    "    c = twint.Config()\n",
    "    #c.Search = tag\n",
    "    c.Username = i\n",
    "    c.Language = 'es'\n",
    "    c.Since = since\n",
    "    c.Until = until\n",
    "    c.Limit = 100\n",
    "    c.Pandas = True\n",
    "    c.Hide_output= True\n",
    "    twint.run.Search(c)\n",
    "    df_twint = twint.storage.panda.Tweets_df\n",
    "    return df_twint\n",
    "\n",
    "#search_tweets('MartinVizcarraC', '2015-05-01', '2015-06-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Se establece el codigo de configuracion para recuperacion los tweets mediante hashtags:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweets_hash(i, since, until):\n",
    "    tag = Buscar_Palabra(i,'Peru')\n",
    "    c = twint.Config()\n",
    "    c.Search = tag\n",
    "    c.Language = 'es'\n",
    "    c.Since = since\n",
    "    c.Until = until\n",
    "    c.Limit = 100\n",
    "    c.Pandas = True\n",
    "    c.Hide_output= True\n",
    "    twint.run.Search(c)\n",
    "    df_twint = twint.storage.panda.Tweets_df\n",
    "    return df_twint\n",
    "\n",
    "#search_tweets('MartinVizcarraC', '2015-05-01', '2015-06-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Mediante el metodo **afinar_busqueda**, se configura la busqueda para que devuelva la cantidad de tweets mayores a 0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afinar_busqueda(i,since,until, parametro):\n",
    "    df_twint = pd.DataFrame()\n",
    "    k = 0\n",
    "    while len(df_twint) < 1:\n",
    "        #print(k)\n",
    "        #df_twint = search_tweets(i, since, until)\n",
    "        df_twint = search_tweets_hash(i, since, until)\n",
    "        if len(df_twint) > 1 or k == 2:\n",
    "            break\n",
    "        k+=1\n",
    "    return df_twint\n",
    "           \n",
    "#afinar_busqueda('MartinVizcarraC', '2021-02-01', '2021-03-01',5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccionar usernames ##\n",
    "*Se define las cuentas verificadas de Twiter para recopilar sus opiniones, en este caso utilizar el user_id. En el caso del escenario politico, los personajes seleccionados fueron los candidatos politicos de las elecciones presidenciales 2021 del Peru. Mientras que en el escenario de la pandemia, las cuentas seleccionadas son las principales entidades de salud en el Peru; asi como personajes relevantes (Ministros de Salud, medicos, etc).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_politica=['PedroCastilloTe','KeikoFujimori','MartinVizcarraC','Vero_Mendoza_F','DanielUrresti1','FSagasti',\n",
    "               'rlopezaliaga1','yonhy_lescano','CesarAcunaP','HDeSotoPeru','George_Forsyth','MerinoDeLama',\n",
    "               'ONPE_oficial', 'JNE_Peru']\n",
    "user_pandemia=['Minsa_Peru', 'ougarteu', 'victorzamora','EsSaludPeru', 'drhuerta', 'SuSaludPeru', 'SISPeruOficial',\n",
    "               'HCevallosFlores', 'ErnesBustamante','maguina_ciro','A_Aguinaga','CMP_PERU', 'CayetanoHeredia',\n",
    "               'EdMalagaTrillo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccionar hashtags ##\n",
    "*Se definen diferentes hashtags que fueron utilizados durante el desarrollo de acontecimientos politicos y de pandemia.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_politico = ['#EleccionesPeru2021','#FranciscoSagasti','#VioletaBermudez','#BicentenarioPeru','#AnaJara',\n",
    "                 '#WaldoMendoza', '#MirthaVásquez', '#felizbicentenarioPeru', '#sagasti', '#FelizDiaPeru', '#FelicesFiestasPatrias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_pandemia = ['#Covid-19','#pandemia','#coronavirus','#vacunas','#VacúnateYa', '#COVID19', '#PongoElHombro', \n",
    "                 '#LaVacunaEsVida', '#vacuna', '#vacunaton', '#NoBajemosLaGuardia',\n",
    "       '#VamosaSalirAdelante', '#vacunacion', '#sialavacuna',\n",
    "       '#VacunaFest', '#TodasLasVacunasSirven', '#VacunaCOVID19',\n",
    "       '#antivacunas', '#NuevaCepa', '#Sinopharm', '#YoApoyoaBeto',\n",
    "       '#ErnestoBustamante', '#EsSaludPeru', '#pandemia', '#covid',\n",
    "       '#covid_19', '#SagastiGenocida', '#SuizaLab', '#AstraZeneca',\n",
    "       '#Vacunate', '#Vacunagate', '#Minsa', '#Pfizer', '#delta',\n",
    "       '#variante', '#PfizerBiontech', '#Moderna', '#VacunacionPeru',\n",
    "       '#VacunateYa', '#Minsa_Peru', '#Omicron', '#yomequedoencasa',\n",
    "       '#SARSCoV2', '#confinamiento', '#cuarentena', '#SaludEnTuVIda', '#mascarillas','#MedidasPreventivas',\n",
    "       '#LavadoDeManos', '#UsoDeCubrebocas', '#SanaDistancia',\n",
    "       '#PrevenirEsSalud','#CuidarteEsCuidarnos', '#SaludMental','#PCR']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_pandemia1 = ['#Covid-19','#pandemia','#coronavirus','#VacúnateYa', '#COVID19', '#PongoElHombro', \n",
    "                 '#LaVacunaEsVida', '#vacunaton', '#NoBajemosLaGuardia',\n",
    "       '#VamosaSalirAdelante', '#vacunacion', '#sialavacuna',\n",
    "       '#VacunaFest', '#TodasLasVacunasSirven', '#VacunaCOVID19',\n",
    "       '#antivacunas', '#NuevaCepa', '#Sinopharm', '#EsSaludPeru', '#covid', '#SuizaLab', '#AstraZeneca',\n",
    "       '#Vacunate', '#Vacunagate', '#Minsa', '#Pfizer', '#delta','#Moderna', '#VacunacionPeru',\n",
    "       '#VacunateYa', '#Minsa_Peru', '#Omicron', '#yomequedoencasa',\n",
    "       '#SARSCoV2', '#confinamiento', '#cuarentena', '#SaludEnTuVIda', '#mascarillas',\n",
    "        '#PongoElHombro', '#VacunaCOVID19', \"#VacunasPorLaVida\" , \"#VacunarseEsNecesario\",\n",
    "       '#LavadoDeManos', '#UsoDeCubrebocas', '#SanaDistancia',\n",
    "       '#PrevenirEsSalud','#CuidarteEsCuidarnos', '#SaludMental','#PCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_pandemia_2 = ['#PongoElHombro', '#VacunaCOVID19', \"#VacunasPorLaVida\" , \"#VacunarseEsNecesario\", '#LavadoDeManos', '#UsoDeCubrebocas', '#SanaDistancia',\n",
    "       '#PrevenirEsSalud','#CuidarteEsCuidarnos', '#SaludMental','#PCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 49 11\n"
     ]
    }
   ],
   "source": [
    "print(len(hash_pandemia), len(hash_pandemia1), len(hash_pandemia_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperacion de Tweets ##\n",
    "*Los tweets que son recuperandos son almacenados en un dataframe pos separado y posteriormente estos son unificados en dataframe general para hacer la limpieza de datos.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Generar_Tweets(lista, Rutabase, year_1, year_2 ):\n",
    "    lista = pd.unique(lista)\n",
    "    numero_tweets = 0\n",
    "    for i in lista:\n",
    "        sub_df = pd.DataFrame()\n",
    "        for j in range(year_1,year_2):\n",
    "            df = pd.DataFrame()\n",
    "            for x in range(1,13):\n",
    "                print(x)\n",
    "                if x < 12:\n",
    "                    since = str(j) + '-'+ str(x) +'-01'\n",
    "                    until = str(j) + '-'+ str(x + 1) +'-01'\n",
    "                else:\n",
    "                    #print('aqui')\n",
    "                    since = str(j) + '-'+ str(x) +'-01'\n",
    "                    until = str(j + 1) + '-'+ str(1) +'-01'\n",
    "                df = afinar_busqueda(i, since, until, 5)\n",
    "                #print(since, until)\n",
    "                    #print(len(df))\n",
    "                sub_df = pd.concat([sub_df, df],ignore_index=False)\n",
    "                Rutarel =  i + '.csv'\n",
    "        Rutasol = os.path.join(Rutabase, Rutarel)\n",
    "        sub_df.to_csv(Rutasol, index = False)\n",
    "        print('Se ha creado correctamente el ' + i + '.csv, recuperando ' + str(len(sub_df)) + ' tweets')\n",
    "        sub_df.to_csv(i + '.csv')\n",
    "#Generar_Tweets(user_pandemia,2021,2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_size = os.path.getsize(r'C:\\test\\file1.txt') \n",
    "def SeleccionarTweets(lista, Rutabase):\n",
    "    total_df = pd.DataFrame() \n",
    "    count = 0\n",
    "    for i in lista:\n",
    "        Rutarel =  i + '.csv'\n",
    "        Rutasol = os.path.join(Rutabase, Rutarel)\n",
    "        if (os.path.getsize(Rutasol) > 2):\n",
    "            print( Rutasol, os.path.getsize(Rutasol))\n",
    "            df = pd.read_csv(Rutasol, delimiter=',')\n",
    "            print(len(df))\n",
    "            new_df = df[df['language'] == 'es']\n",
    "            print('Se ha recuperado ' + str(len(new_df)) + ' tweets en el idioma espanhol')\n",
    "            new_df = new_df.reset_index(drop = True)\n",
    "            #new_df = Eliminar_Repetidos(new_df, 'tweet', 'spanish', 0.5)\n",
    "            #print('Se han eliminado los repetidos del archivo '+ i +'.csv'+ ' , el nuevo numero de tweets es ' + str(len(new_df)))\n",
    "            #count = count + len(df)\n",
    "            total_df = pd.concat([total_df, new_df],ignore_index=False)\n",
    "    #print(total_df.shape)\n",
    "    total_df = total_df.reset_index(drop = True)\n",
    "    print('Se ha recuperado ' + str(len(total_df)) + ' tweets en total' )\n",
    "    return total_df\n",
    "   \n",
    "#def Eliminar_Reptidos_Archivo(df):\n",
    "    \n",
    "#df[(df['date'] > '2021-04-01') & (df['date'] < '2021-10-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recuperar_Indice(lista_indices):\n",
    "    index = []\n",
    "    for i in lista_indices:\n",
    "        index.append(i[1])\n",
    "    index = pd.unique(index)\n",
    "    index = list(index)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminar duplicados##\n",
    "*Se establece la **similidaridad de coseno** para identificar el grado de similitud entre dos tweets. Este metodo sirve para eliminar los tweets duplicados.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eliminar_Repetidos(df, label, language, threshold):\n",
    "    tfidf = TfidfVectorizer(stop_words = stopwords.words(language))\n",
    "    # Construct the TF-IDF matrix\n",
    "    tfidf_matrix = tfidf.fit_transform(df[label])\n",
    "    #print(shape(tfidf_matrix))\n",
    "    # Generate the cosine similarity matrix\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    #print(cosine_sim)\n",
    "    #Lista que recupera los tweets\n",
    "    A = []\n",
    "    for i in range(0,len(cosine_sim)):\n",
    "        for k in range(0,len(cosine_sim)):\n",
    "            if ((k>i) and (cosine_sim[i][k] >= threshold)):\n",
    "                #print(i,k)\n",
    "                A.append([i,k])\n",
    "                \n",
    "    indices_tweets = Recuperar_Indice(A)\n",
    "    df = df.drop(indices_tweets)\n",
    "    #print(df)\n",
    "    df = df.reset_index(drop = True)\n",
    "    print('Se han recuperado ' + str(len(df)) + ' tweets')\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rutabase = r'C:/Users/USER/Documents/Git/SATwint/'\n",
    "Rutabase =  r'C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/'\n",
    "#nam3e_carpeta = 'pandemia/'\n",
    "def Obtener_dataset(lista_tweet, name_carpeta, label, language, name, year_1, year_2):\n",
    "    #Crear_carpeta(Rutabase, name_carpeta)\n",
    "    Ruta = Rutabase + name_carpeta\n",
    "    #Generar_Tweets(lista_tweet, Ruta, year_1, year_2)\n",
    "    df = SeleccionarTweets(lista_tweet, Ruta)\n",
    "    df.to_csv(name + '.csv') \n",
    "    df = Eliminar_Repetidos(df, label, language, 0.5)\n",
    "    df.to_csv(name + 'Limpio.csv')  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#Covid-19.csv 255692\n",
      "419\n",
      "Se ha recuperado 329 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#pandemia.csv 248064\n",
      "352\n",
      "Se ha recuperado 329 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#coronavirus.csv 208913\n",
      "314\n",
      "Se ha recuperado 250 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#COVID19.csv 302565\n",
      "451\n",
      "Se ha recuperado 342 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#NoBajemosLaGuardia.csv 50704\n",
      "78\n",
      "Se ha recuperado 74 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#VamosaSalirAdelante.csv 8525\n",
      "13\n",
      "Se ha recuperado 13 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#vacunacion.csv 69413\n",
      "103\n",
      "Se ha recuperado 93 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#VacunaCOVID19.csv 307137\n",
      "463\n",
      "Se ha recuperado 422 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#antivacunas.csv 2899\n",
      "4\n",
      "Se ha recuperado 4 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#NuevaCepa.csv 41160\n",
      "51\n",
      "Se ha recuperado 51 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#Sinopharm.csv 47602\n",
      "75\n",
      "Se ha recuperado 47 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#EsSaludPeru.csv 22101\n",
      "29\n",
      "Se ha recuperado 26 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#covid.csv 169553\n",
      "252\n",
      "Se ha recuperado 182 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#SuizaLab.csv 680\n",
      "1\n",
      "Se ha recuperado 1 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#AstraZeneca.csv 77128\n",
      "113\n",
      "Se ha recuperado 104 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#Vacunate.csv 6565\n",
      "9\n",
      "Se ha recuperado 8 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#Minsa.csv 281107\n",
      "380\n",
      "Se ha recuperado 369 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#Pfizer.csv 31842\n",
      "45\n",
      "Se ha recuperado 30 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#delta.csv 5160\n",
      "7\n",
      "Se ha recuperado 4 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#Moderna.csv 24055\n",
      "34\n",
      "Se ha recuperado 28 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#VacunacionPeru.csv 969\n",
      "1\n",
      "Se ha recuperado 0 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#Minsa_Peru.csv 74434\n",
      "109\n",
      "Se ha recuperado 107 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#yomequedoencasa.csv 237649\n",
      "317\n",
      "Se ha recuperado 261 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#SARSCoV2.csv 226098\n",
      "304\n",
      "Se ha recuperado 143 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#confinamiento.csv 127814\n",
      "171\n",
      "Se ha recuperado 164 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#cuarentena.csv 257773\n",
      "340\n",
      "Se ha recuperado 297 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#mascarillas.csv 133331\n",
      "181\n",
      "Se ha recuperado 161 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#VacunaCOVID19.csv 307137\n",
      "463\n",
      "Se ha recuperado 422 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#VacunarseEsNecesario.csv 25185\n",
      "36\n",
      "Se ha recuperado 35 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#LavadoDeManos.csv 72392\n",
      "92\n",
      "Se ha recuperado 82 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#SanaDistancia.csv 41006\n",
      "57\n",
      "Se ha recuperado 56 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#PrevenirEsSalud.csv 11890\n",
      "15\n",
      "Se ha recuperado 13 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#CuidarteEsCuidarnos.csv 112805\n",
      "155\n",
      "Se ha recuperado 149 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#SaludMental.csv 361861\n",
      "515\n",
      "Se ha recuperado 477 tweets en el idioma espanhol\n",
      "C:/Users/claud/OneDrive/Documentos/GITHUB/TESIS/Extraction_with_Twint/Pandemia_hash/#PCR.csv 51655\n",
      "74\n",
      "Se ha recuperado 62 tweets en el idioma espanhol\n",
      "Se ha recuperado 5135 tweets en total\n",
      "Se han recuperado 3600 tweets\n"
     ]
    }
   ],
   "source": [
    "final_df = Obtener_dataset(hash_pandemia,'Pandemia_hash/', 'tweet', 'spanish', 'df_pandemia_hashv2', 2020,2022)\n",
    "#final_df = Obtener_dataset(user_pand,'pandemia/', 'tweet', 'spanish', 'df_elecciones' )\n",
    "#final_df.to_csv('new_pand.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place',\n",
       "       'tweet', 'language', 'hashtags', 'cashtags', 'user_id', 'user_id_str',\n",
       "       'username', 'name', 'day', 'hour', 'link', 'urls', 'photos', 'video',\n",
       "       'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'quote_url',\n",
       "       'search', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
       "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
       "       'trans_dest'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"['coronavirus']\", \"['coronavirus', 'tusaludprimero']\",\n",
       "       \"['coronavirus', 'minsa']\", ..., \"['buenanoticia', 'chorrillos']\",\n",
       "       \"['samu']\",\n",
       "       \"['trastornodellenguaje', 'saludmental', 'derechodetodos']\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['hashtags'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>timezone</th>\n",
       "      <th>place</th>\n",
       "      <th>tweet</th>\n",
       "      <th>language</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>cashtags</th>\n",
       "      <th>...</th>\n",
       "      <th>geo</th>\n",
       "      <th>source</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>translate</th>\n",
       "      <th>trans_src</th>\n",
       "      <th>trans_dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, conversation_id, created_at, date, timezone, place, tweet, language, hashtags, cashtags, user_id, user_id_str, username, name, day, hour, link, urls, photos, video, thumbnail, retweet, nlikes, nreplies, nretweets, quote_url, search, near, geo, source, user_rt_id, user_rt, retweet_id, reply_to, retweet_date, translate, trans_src, trans_dest]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 38 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[final_df['hashtags'] == 'coronavirus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
